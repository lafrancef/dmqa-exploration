{
  "name": "Investigation of the DeepMind Question Answering corpus",
  "tagline": "",
  "body": "The [DMQA](https://github.com/deepmind/rc-data) corpus contains about 310 000\r\nnews articles from CNN and the Daily Mail. It's ostensibly used to train\r\nquestion-answering models, but an interesting feature of the corpus is that\r\neach article contains a few 'highlights': short sentences that are supposed to\r\npresent the main points of the article. It might therefore be possible to use\r\nthis corpus for automated summarization, using the existing highlights for\r\nsupervised training. I set out to investigate this and see what I can come up\r\nwith in a short period of time (~2 weeks).\r\n\r\n## Preliminaries\r\n\r\nI started by trying to familiarize myself with the corpus and its contents.\r\nBefore even reading any articles, I had a few questions that I wanted to answer:\r\n\r\n  * What do the highlights 'look like'? How long are they, how many of them are\r\n    there per article?\r\n  * How much can the highlights stand alone? For example, if you tweeted one\r\n    of these and a link to the article, would the tweet make sense?\r\n  * Do the highlights satisfactorily summarize the article?\r\n  * Is there anything we can do to extract highlights?\r\n\r\nWith those questions in mind, I read quite a few articles from the corpus, to\r\nget a feel for the kind of data I was working with. The contents and highlights\r\nof each article are stored in a text file. Here's an example of one such (very\r\nshort) file:\r\n\r\n```\r\nThe U.N. General Assembly voted Tuesday in favor of a resolution calling for an\r\nend to the economic, commercial and financial embargo imposed by the United\r\nStates against Cuba.\r\n\r\nThere were 186 member states in favor of the resolution, two against, and three\r\nabstentions.\r\n\r\nThe vote is non-binding. It was the 20th consecutive year that the United\r\nNations has voted to condemn the U.S. embargo, which was put in place in 1960.\r\n\r\nThe two opposing votes came from the United States and Israel.\r\n\r\n@highlight\r\n\r\nFor the 20th consecutive year, a General Assembly resolution urges an end to the\r\nembargo\r\n\r\n@highlight\r\n\r\nOnly the United States and Israel vote against the measure; 186 nations vote in\r\nfavor\r\n```\r\n\r\nThe format is pretty simple: first the contents of the article, then several\r\nhighlights, delimited by `@highlight`. About 90% of articles have between 2\r\nand 5 highlights, and the average length of highlights is about 50 words per\r\ndocument.\r\n\r\nAt first sight, it seems that highlights can mostly stand on their own,\r\nespecially if the title is added. For example, the title of the article above is\r\n*U.N. again calls for end of U.S. embargo on Cuba*. It's not unreasonable to\r\nimagine a tweet with the following contents: *U.N. again calls for end of U.S.\r\n embargo on Cuba: only the United States and Israel vote against the measure;\r\n186 nations vote in favor.* (which is incidentally just below the 140\r\ncharacter limit). Highlights however generally contain referents that might be\r\nunclear without a bit of context coming either from the title or from other\r\nhighlights. One example is *the measure* above, where the reference can be\r\nfound in the title. Another example is the following pair of highlights:\r\n\r\n  * *Spokesman for hospital says Casey Kasem being treated for wounds, blood\r\n    pressure issues.*\r\n  * *He had been at a friend's home in Washington state after his wife took him\r\n    there.*\r\n\r\nIn this case, it's unclear what 'he', 'his' and 'him' refer to in the second\r\nhighlight if we don't have the first highlight. However, having all the\r\nhighlights together might form a 'proper' summary that can be read from\r\nbeginning to end.\r\n\r\n## Highlights as summaries\r\n\r\nWe would like to know how well an article is summarized by its 2 - 5 highlights.\r\nAfter reading quite a few articles, I found that highlights could be roughly\r\nclassified in the following categories:\r\n\r\n  * The highlight is a rephrasal of one sentence from the article.\r\n  * The highlight uses chunks of two or more sentences together.\r\n  * The highlight is entirely abstractive, and does not take direct formulations\r\n    from any sentence in the article.\r\n\r\nOf course, this is merely some hand observation, not actual data. We'd like to\r\nknow in general, given a highlight, what are the most similar sentences to it\r\nin the article. We must therefore define what 'similarity' means. In this case,\r\nn-gram overlap is a straightforward measure to use. I used ROUGE-2 specifically.\r\nThe details are in [0], but basically, ROUGE-N calculates the recall\r\nof bigrams between a reference and a candidate (i.e. it is maximized when the\r\ncandidate contains all the bigrams in the reference).\r\n\r\nWith this in mind, we can then look at the articles to determine which sentences\r\nare most similar to each highlight of the article. There are two things we\r\nmight be interested in at first:\r\n\r\n  1. The rank of the sentences deemed similar to the highlights. If we assume\r\n  that the highlights are important points, then similar sentences are also\r\n  important. Therefore knowing the rank of similar sentences gives us an idea\r\n  of where the most important sentences of an article are located.\r\n  2. The 'shape' of the distribution of similar sentences for each highlight.\r\n  For example, some highlights may be very similar to one sentence, and only\r\n  superficially similar to a few others. This goes back to the point I observed\r\n  earlier that highlights can be classified in various categories according to\r\n  how they pick information from the text.\r\n\r\n### Rank of similar sentences\r\n\r\nI computed the most similar sentences of a document to each highlight, and\r\ndumped their rank and associated similarity with the highlight. Instead of\r\nusing the rank directly, however, I normalized it with the document length.\r\n\r\nLooking only at the most similar sentence for each highlight (in other words,\r\nthe 'best match' for a highlight), we have the following distribution:\r\n\r\n![Frequency of the most similar sentence to a highlight by rank in the article.]\r\n(docs/rankfreq.png)\r\n\r\nSince we are dealing with news articles, this is completely unsurprising. A\r\nhighlight's most similar sentence has a very large chance of being in fact the\r\nfirst sentence in the article; afterwards, this chance decays exponentially. A\r\npoint of note is the presence of small spikes around the halfway mark and the\r\nend. Since over 70% of articles have either 3 or 4 highlights, it seems that\r\nsome of the writers simply decided to pick the first, middle and last sentences\r\nof their articles as highlights.\r\n\r\nAn important point not seen on the graph is that in fact, 11% of highlights\r\ndo not overlap at all with any sentences in their respective articles, which\r\nmeans they were purely abstractive. This is the most common class of highlight.\r\nIn fact, if we consider a highlight to be 'extractive' only if it has a best\r\nsimilarity score of at least 0.3 (meaning that 30% of the bigrams in the\r\nhighlight are found in the best-matching sentence), then nearly half (48%) of\r\nthe highlights are not extractive. The choice of a similarity score of 0.3 is\r\nquite arbitrary, but illustrates the fact that a large amount of the highlights\r\nare only superficially similar to their associated articles.\r\n\r\n### Shape of highlight similarities\r\n\r\nAfter toying around with the data gathered in the previous section, I looked at\r\nthe 'shape' of sentence similarities for each highlight. For example, a given\r\nhighlight might have the following similarity sequence:\r\n\r\n```[(7, 0.83), (37, 0.33), (1, 0.16)]```\r\n\r\nIn this case, the highlight had 83% overlap with sentence 7 of the article, 33%\r\noverlap with sentence 37 and 16% overlap with sentence 1. In this case we can\r\nreasonably conclude, without even looking at the article, that the highlight\r\nwas probably just a shortened version of sentence 7, and the overlap with other\r\nsentences is mostly coincidental. Looking at the actual data confirms this:\r\n\r\n  * Highlight: But some of the largest players in the consumer electronics\r\n    industry are shunning CES.\r\n  * Sentence 7: But with some of the largest players in today's consumer\r\n    electronics industry shunning CES, the trade show's impact may be waning.\r\n\r\nOn the other hand, we could expect a highlight which draws equally from many\r\nsentences to not actually be extractive, but really to just coincidentally\r\noverlap the sentences in question.\r\n\r\nTherefore, I tried to see the different kinds of overlap behaviors. The idea is\r\nto cluster the similarity scores of highlights by shape similarity. I wanted to\r\nhave a way of comparing similarity sequences that was relatively invariant to\r\nthe actual values of the sequence; therefore, I decided to do clustering based\r\non deltas between successive values. In other words, the similarity sequence\r\n`[0.75, 0.5, 0.5, 0.2]` was transformed to `[-0.25, 0, -0.3]`. Sequence\r\ndistance was set to be the sum of squared differences between pairs of elements.\r\n\r\nTo cluster sequences, I simply used a greedy algorithm wherein each cluster had\r\na representative and sequences were either added to an existing cluster or a new\r\none based on a threshold on the sequence distance. Here I could have also used\r\nfor example a density-based clustering algorithm such as DBSCAN. In this case\r\nthe well-known k-means doesn't apply very well since we don't know the number\r\nof clusters we are looking for.\r\n\r\nAfter clustering, I decided to ignore clusters with less than one percent of the\r\nhighlights (there are about a million total highlights, so I discarded clusters\r\nwith less than 10'000 elements). Again, this is fairly arbitary, but we're\r\ninterested in general tendencies. This left me with 12 clusters which contained\r\nover 90% of the highlights. A few of those clusters were also similar in\r\nsequence behavior to each other, the only thing changing being the number of\r\nelements in the sequence. For example, the following sequences were in different\r\nclusters:\r\n\r\n  * `[0.33, 0.33, 0.33, 0.33, 0.16, 0.16, 0.16, 0.16, 0.16]`\r\n  * `[0.22, 0.22, 0.22, 0.11, 0.11]`\r\n\r\nHowever, they share a very similar behavior, namely that they draw from a large\r\namount of sentences (remember that the average length of a highlight is about 50\r\ncharacters, so no more than a few words - therefore, even overlapping with three\r\nsentences is already a large amount), at relatively low levels. This means that\r\nthey're not that fundamentally different, and in reality the reason why the\r\nfirst sequence has more elements could be simply because the article in question\r\nwas longer.\r\n\r\nBroadly, I classified the sequences in four categories from the clusters:\r\n\r\n  * Very similar to one sentence (overlap above 0.7), and barely if at all\r\n    similar to any other sentence. (about 185'000 highlights)\r\n  * Only somewhat similar to one sentence (overlap around 0.4 to 0.6), and\r\n    barely similar to any other sentence. (about 365'000 highlights)\r\n  * Very similar to two sentences (about 150'000 highlights)\r\n  * Not very similar to any sentence, or equally similar to many sentences,\r\n    indicating that the similarity is probably coincidental (about 350'000\r\n    sentences).\r\n\r\nAgain, this is fairly predictable, but it's good to have it confirmed. An\r\ninteresting point to note is that the third category, highlights that are\r\nvery similar to two sentences, do not necessarily draw different information\r\nfrom both sentences. Sometimes, this can be caused by image captions still in\r\nthe text of the article, which themselves have similar text to a sentence of the\r\narticle. At other times, it can be caused by an organization/place name that is\r\nquite long and thus takes most of the highlight, while also being used multiple\r\ntimes in the article text. This seems to point to the fact that most highlights\r\nmainly draw from one sentence at most, which seems reasonable given the fact\r\nthat highlights are so short.\r\n\r\n## Discussion and future work\r\n\r\nThis project was over a short period of time, so I didn't get into all the areas\r\nI wanted to explore. Still, exploring this dataset allowed me to make some\r\ninteresting observations. Highlights in general are only superficially\r\nextractive; when they are, they predictably take their content from the\r\nsentences closer to the beginning of the article. Therefore, simple overlap\r\nmeasures seem inadequate to measure the 'quality' of a highlight. To go further,\r\nI would have to try using a metric that's more 'semantic' in nature.\r\n\r\nOne task that remains in the exploration of the corpus is to properly evaluate\r\nwhether the collection of highlights for an article provide an adequate summary.\r\nI cursorily glanced over a few articles and found that that seemed to be\r\ngenerally the case, but it would be good to make sure. For that, one could\r\nrun a known summarization algorithm on the articles, and then compare the\r\nresults with the highlights. The reasoning is that we know in general the\r\nperformance of a given algorithm, so we can compare it with the performance\r\nobtained on this dataset. If we get results that are markedly different than\r\nwith other corpora, this could give us a hint about whether the highlights are\r\nadequate summaries or not.\r\n\r\nHowever, interpreting the results could be challenging; if the algorithm\r\nperforms more poorly under the metric we use, is it because the highlights don't\r\nform a good summary or because the the metric is not adapted to highlights?\r\nSimilary, if the algorithm performs better, is it because the highlights *do*\r\nform a good summary or because they 'overfit' the measure? In this case, perhaps\r\nthe best way is good old-fashioned human judgement.\r\n\r\nIt would also be interesting to try extracting highlights. This is in fact\r\nvery similar to a summarization problem, except that the output is very short\r\nand only needs to capture one interesting point of the article (as opposed to\r\nits entire point). The method I would like to try on this is documented in [1].\r\nThe general idea is to cluster sentences in the document based on semantic\r\nsimilarity. Every cluster is to be realized as one output sentence. In our case,\r\nevery cluster would be a potential highlight. The actual language generation is\r\ndone by a Markov process where each step indicates which word to generate, based\r\non a bigram model of the cluster. This seems like an interesting avenue to try\r\nbecause clustering an article based on sentence gives us several clusters from\r\nwhich we are then free to pick, depending on how many highlights we'd like to\r\ngenerate.\r\n\r\n## References\r\n\r\n[0] Lin, C. Y. (2004, July). Rouge: A package for automatic evaluation of\r\nsummaries. In *Text summarization branches out: Proceedings of the ACL-04\r\nworkshop* (Vol. 8).\r\n\r\n[1] Murray, G. (2015, June). Abstractive meeting summarization as a Markov\r\ndecision process. In *Canadian Conference on Artificial Intelligence*\r\n(pp. 212-219). Springer International Publishing.",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}